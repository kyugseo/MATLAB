{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The classification of raw text in natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Artificial intelligence has improved significantly. Users can run artificial intelligence programs on older computer systems. On the other hand, the beneficial effects of machine learning are tremendous. Natural Language Processing is a branch of AI  and linguistics that studies the interaction between computers and human languages. which means that machines have the ability to read, understand and convey meaning. NLP has been very successful in healthcare, media, finance and human resources.\n",
    "This text classification can be sorted out: Rule-based System, Machine System, Hybrid System.\n",
    "In this python project, I will present following topic:\n",
    "\n",
    "* [Method](#Method)\n",
    "* [Loading the data set](#Loading-the-data-set)\n",
    "* [Extracting features from text files](#Extracting-features-from-text-files)\n",
    "* [Running ML algorithms](#Running-ML-algorithms)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "A rule-based approach uses a manually crafted set of language rules to classify text into organized groups. These rules instruct the system to identify relevant categories based on their content using semantic relevant elements of a text. Each rule consists of a leading or pattern and a prediction category.\n",
    "Rule-based systems can be understood by humans and can be improved over time. However, this approach has several drawbacks. First, generating rules for complex systems can be quite challenging and is time-consuming, usually requiring a lot of analysis and testing.\n",
    "\n",
    "On the other hand, machine learning text classification learns to classify based on historical observations. By using prelabeled examples as training data, machine learning algorithms can learn different associations between text sequence, and learn that certain outputs. There are two ways to do this, by using deep learning libraries tensorflow, sklearn.feature_extraction.text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data set\n",
    "\n",
    "There are two ways to do this: tensorflow and scikit-learn.\n",
    "Both example is loading the data from imdb. Which is the movie review platform and i will classify the review tells 'positive' or 'negative' for the movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "    path='imdb_word_index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  19.0M      0  0:00:04  0:00:04 --:--:-- 19.0M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab\tREADME\ttest  train\r\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\r\n",
      "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/6248_7.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text files\n",
    "\n",
    "Text files are actually series of words (ordered). In order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model for our example. Briefly, we segment each text file into words (for English splitting by space), and count # of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).\n",
    "\n",
    "Next, we can use the [`CountVectorizer`] provided by the [`scikit-learn`] library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.keras.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we can remove punctuation and make the text all in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'not', 'a', 'sentence']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"<div>This is not a sentence.<\\div>\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_train_test_imdb_data(data_dir):\n",
    "    \"\"\"Loads the IMDB train/test datasets from a folder path.\n",
    "    Input:\n",
    "    data_dir: path to the \"aclImdb\" folder.\n",
    "    \n",
    "    Returns:\n",
    "    train/test datasets as pandas dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        data[split] = []\n",
    "        for sentiment in [\"neg\", \"pos\"]:\n",
    "            score = 1 if sentiment == \"pos\" else 0\n",
    "\n",
    "            path = os.path.join(data_dir, split, sentiment)\n",
    "            file_names = os.listdir(path)\n",
    "            for f_name in file_names:\n",
    "                with open(os.path.join(path, f_name), \"r\", encoding=\"utf-8\") as f:\n",
    "                    review = f.read()\n",
    "                    data[split].append([review, score])\n",
    "\n",
    "    np.random.shuffle(data[\"train\"])        \n",
    "    data[\"train\"] = pd.DataFrame(data[\"train\"],\n",
    "                                 columns=['text', 'sentiment'])\n",
    "\n",
    "    np.random.shuffle(data[\"test\"])\n",
    "    data[\"test\"] = pd.DataFrame(data[\"test\"],\n",
    "                                columns=['text', 'sentiment'])\n",
    "\n",
    "    return data[\"train\"], data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = load_train_test_imdb_data(\n",
    "    data_dir=\"aclImdb/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>cat</th>\n",
       "      <th>day</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test sentence</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bad  cat  day  good\n",
       "test sentence    0    0    2     1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_texts = [ \"This is a good cat\", \"This is a bad day\" ]\n",
    "\n",
    "test_texts = [\"This day is a good day\"]\n",
    "\n",
    "# this vectorizer will skip stop words\n",
    "vectorizer = CountVectorizer( stop_words=\"english\",\n",
    "                                preprocessor=clean_text)\n",
    "\n",
    "# fit the vectorizer on the training text\n",
    "vectorizer.fit(training_texts)\n",
    "\n",
    "# get the vectorizer's vocabulary\n",
    "inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "vocabulary = [inv_vocab[i] for i in range(len(inv_vocab))]\n",
    "\n",
    "# vectorization example\n",
    "pd.DataFrame(\n",
    "    data=vectorizer.transform(test_texts).toarray(),\n",
    "    index=[\"test sentence\"],\n",
    "    columns=vocabulary\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>cat</th>\n",
       "      <th>day</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test sentence</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bad  cat  day  good\n",
       "test sentence    1    1    0     0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts1 = [\"This cat is a bad\"]\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=vectorizer.transform(test_texts1).toarray(),\n",
    "    index=[\"test sentence\"],\n",
    "    columns=vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal  use [wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz] to download the dataset from Stanford’s website. \n",
    "\n",
    "And to extract this, [tar -zxvf aclImdb_v1.tar.gz]\n",
    "\n",
    "We have a data folder called aclImdb.\n",
    "\n",
    "I will use classification algorithm, which in this case was a linear support vector machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the IMDB dataset: 83.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",preprocessor=clean_text)\n",
    "\n",
    "training_features = vectorizer.fit_transform(train_data[\"text\"])    \n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "\n",
    "# Training\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the IMDB dataset: 88.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",preprocessor=clean_text,ngram_range=(1, 2))\n",
    "\n",
    "training_features = vectorizer.fit_transform(train_data[\"text\"])    \n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "\n",
    "# Training\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data classification tools work to enhance the capabilities of everyday life that related to data. In some other cases, classifiers are used by people who need data processing. For example, companies can sort customers by their target. Or, an example would be spam filtering in daily life. We have been using natural language processing without noticing, and it is developing day by day.  Through this project, we did the work of vectorization letters and classifying them into categories.This is the first step the computer will process when using artificial intelligence or other Internet services in the future, and the more it improves, the easier it will be to get more accurate information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the classical methods (Fisher discriminan) are extremely useful, they no longer perform well or even break down in high dimensional setting. A common feature of many contemporary classification problems is that the dimensionality of the feature vector is much larger than the available training sample size n. The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference\n",
    "\n",
    "“Natural Language Processing.” Wikipedia, Wikimedia Foundation, 26 Feb. 2021, en.wikipedia.org/wiki/Natural_language_processing.\n",
    "\n",
    "Team, Keras. “Keras Documentation: Text Classification from Scratch.” Keras, keras.io/examples/nlp/text_classification_from_scratch/. \n",
    "\n",
    "Text Classification: The First Step Toward NLP Mastery\n",
    "https://medium.com/data-from-the-trenches/text-classification-the-first-step-toward-nlp-mastery-f5f95d525d73\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
